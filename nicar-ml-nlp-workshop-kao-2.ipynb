{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICAR Workshop: Machine Learning and NLP\n",
    "\n",
    "By Jeff Kao, ProPublica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Wow, that was a lot of dataviz and ML! We're finally on our way to using that for NLP.\n",
    "\n",
    "I'll be posting these notebooks on [propublica's github](https://github.com/propublica) for your future use & play.\n",
    "\n",
    "### What this is.\n",
    "\n",
    "How can we use ML/NLP to help us break down and investigate larger datasets? How can we gain an intuitive sense of 'how it works' without having to dig deeply into the math?\n",
    "\n",
    "Instead of learning it from the ground (math) up, let's get a top-down understanding with the help of data visualization.\n",
    "\n",
    "Goals for this session:\n",
    "* gain an intuitive understanding for unsupervised machine learning\n",
    "* connect machine learning to NLP\n",
    "* learn to incorporate these techniques into your investigations\n",
    "\n",
    "### What this is not.\n",
    "\n",
    "Machine learning is not magic. While these are useful tools in a data journalist's repertoire, they don't replace what we are already good at: understanding the context & real world interactions underlying the data. It also doesn't replace the 'traditional' statistical techniques we already have.\n",
    "\n",
    "This session will NOT be:\n",
    "* overly math-y (although a deeper understanding of the math helps you get better results)\n",
    "* about algorithmic bias (although one should always be aware of the gaps between the real world and its representation in data)\n",
    "* about supervised machine learning (there are already a ton of online resources dedicated to that)\n",
    "* about when machine learning is useful (although that is great to learn too)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ./data/ira_tweets_csv_hashed.zip -d ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ./data/ira_users_csv_hashed.zip -d ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv('./data/ira_users_csv_hashed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_eng = df_users[df_users['account_language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_all = pd.read_csv('./data/ira_tweets_csv_hashed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['tweet_language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_eng = df_all[df_all['tweet_language'] == 'en']\n",
    "df_eng = df_eng[df_eng['userid'].isin(df_users_eng['userid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_red_cols = df_eng[['tweetid', 'userid',\n",
    "       'tweet_text', 'tweet_time', 'tweet_client_name', \n",
    "       'in_reply_to_tweetid', 'in_reply_to_userid', 'quoted_tweet_tweetid', \n",
    "       'quote_count', 'reply_count', 'like_count', 'retweet_count',\n",
    "       'is_retweet', 'retweet_userid', 'retweet_tweetid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_by_user = \\\n",
    "(df_eng_red_cols[['userid','tweet_text']]\n",
    " .groupby('userid')\n",
    " .agg({'userid': 'first', 'tweet_text': lambda x: ' '.join(x)})\n",
    " .set_index('userid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_by_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_eng_by_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_counts = count_vect.fit_transform(df['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = (pd.DataFrame\n",
    "             .from_dict(count_vect.vocabulary_, orient='index')\n",
    "             .rename(columns={0: 'index'})\n",
    "             .sort_values(by='index'))\n",
    "df_counts['count'] = np.array(X_counts.sum(axis=0)).flatten()\n",
    "df_counts = df_counts.drop(columns=['index'])\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.set_title('Top 80 tokens')\n",
    "df_counts.sort_values('count', ascending=False)[:80].sort_values('count').plot.barh(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_tfidfs = tfidf_vect.fit_transform(df['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_lsa = lsa.fit_transform(X_tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lsa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance of the SVD step: {int(explained_variance * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lsa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Separate out the features\n",
    "x = X_lsa\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "um = reducer.fit_transform(x)\n",
    "df_um = pd.DataFrame(\n",
    "    data = um,\n",
    "    columns = ['um1', 'um2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_um.index = df_eng_by_user.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=1,\n",
    "    figsize=(15,15)\n",
    ")\n",
    "sns.scatterplot(data=df_um, y='um2', x='um1', alpha=0.1, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Separate out the features\n",
    "x = X_tfidfs\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "um = reducer.fit_transform(x)\n",
    "df_um = pd.DataFrame(\n",
    "    data = um,\n",
    "    columns = ['um1', 'um2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=1,\n",
    "    figsize=(15,15)\n",
    ")\n",
    "sns.scatterplot(data=df_um, y='um2', x='um1', alpha=0.1, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have documents in space -- we can visualize or use clustering algos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: NLP and Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning requires labelled data, but often when we are doing investigations, we don't yet know what we are looking for. Unsupervised learning helps our analysis because we let the numbers take us in the right direction.\n",
    "* K-means\n",
    "* HDBSCAN\n",
    "* IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing: Turning words to numbers\n",
    "\n",
    "* bag-of-words (discuss bag-of-characters and n-grams)\n",
    "* tfidf\n",
    "* LSI\n",
    "* we won't have time for:\n",
    "* word2vec and other deep-learning based language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing: Clustering and outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
